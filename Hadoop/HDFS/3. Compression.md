
# 3. Hadoop 压缩框架

> Hadoop 支持的压缩格式：


| 压缩格式 | 算法    | 文件扩展名 | 支持多文件 | 可分割 |
| -------- | ------- | ---------- | ---------- | ------ |
| DEFLATE  | DEFLATE | .deflate   | 否         | 否     |
| gzip     | DEFLATE | .gz        | 否         | 否     |
| zip      | DEFLATE | .zip       | 是         | 是     |
| bzip     | bzip2   | .bz2       | 否         | 是     |
| LZO      | LZO     | .lzo       | 否         | 否     |

- **CompressionCodecFactory** 工厂类，采用 **工厂方法** 的设计模式，根据应用程序的实际需求，构建对应的 **CompressionCodec** 实现类的对象。

```java
public class CompressionCodecFactory {
    //...
    public CompressionCodec getCodec(Path file){
       // ...
    }
    public CompressionCodec getCodecByClassName(String classname){
       // ...
    }
    public CompressionCodec getCodecByName(String codecName) {
       // ...
    }
   // ...
}
```

- **CompressionCodec** 接口，采用 **抽象工厂** 的设计模式，其实现类需要构造并返回对应的 **CompressionOutputStream**、**CompressionInputStream**、**Compressor**、**Decompressor** 四个接口的实现类的对象

```java
//采用 抽象工厂 设计模式
public interface CompressionCodec {
    //压缩有关
    CompressionOutputStream createOutputStream(OutputStream out);
    CompressionOutputStream createOutputStream(OutputStream out, Compressor compressor)
    Class<? extends Compressor> getCompressorType();
    Compressor createCompressor();
    //解压缩有关
    CompressionInputStream createInputStream(InputStream in);
    CompressionInputStream createInputStream(InputStream in, Decompressor decompressor);
    Class<? extends Decompressor> getDecompressorType();
    Decompressor createDecompressor();
    //获得对应文件的扩展名
    String getDefaultExtension();
}
```

- **Compressor** 和 **Decompressor**：压缩器和解压缩器。 

- **CompressionOutputStream** 和 **CompressionInputStream** ：压缩流和解压缩流，提供了基于流的压缩和解压缩能力。

- **CompressorStream** 和 **DecompressorStream** 利用压缩器 **Compressor** 和解压缩器 **Decompressor** 实现了一个通用的压缩流/解压缩流，在Hadoop中引入一个新的压缩算法，如果没有特殊的考虑，只需要实现相关的 **Compressor** 和 **Decompressor** 接口，然后通过 **CompressorStream** 和 **DecompressorStream** 就能实现相关压缩算法的输入/输出流。

> **Compressor** 的使用过程：

```java
public class CompressDemo {
	public static void compressor() throws IOException {
		//读入要被压缩的内容
		File fileIn = new File("E:/data.csv");
		InputStream in = new FileInputStream(fileIn);
		int dataLength = in.available();
		byte[] inbuf = new byte[dataLength];
		in.read(inbuf, 0, dataLength);
		in.close();
		//长度受限制的输出缓冲区，用于说明 finished() 方法：读取压缩数据，每次最多只能读取的大小
		int compressorOutputBufferSize = 2048;
		byte[] outbuf = new byte[compressorOutputBufferSize];
		//压缩器
		Compressor compressor = new Bzip2Compressor();
		int step = 100; //每次 setInput() 的字节大小
		int inputPos = 0;	// inbuf 当前位置
		int putCount = 0;   // setInput() 调用次数 
		int getCount = 0;	// compress() 有效调用次数（返回值>0）
		int compressedLen = 0;	//读取到的压缩数据的长度
		while(inputPos < dataLength){
			//进行多次 setInput()，将数据写入 compressor 的内部缓冲区
			int len = Math.min(dataLength-inputPos, step);
			compressor.setInput(inbuf, inputPos, len);
			putCount++;
			while(!compressor.needsInput()){
				//compressor 的内部缓冲区满了，需要调用 compress 方法获取压缩后的数据，释放缓冲区空间。
				compressedLen = compressor.compress(outbuf, 0, compressorOutputBufferSize);
				if(compressedLen > 0) {
					//读取到了数据
					getCount++;
				}
			}
			inputPos += len;
		}
		compressor.finish(); //通知压缩器所有数据已经写入
		while(!compressor.finished()){
			// 压缩器中还有未读取的压缩数据
			getCount++;
			compressor.compress(outbuf, 0, compressorOutputBufferSize);
		}
		compressor.end(); //关闭并放弃所有未处理的输入
	}
}
```

> **CompressorStream** 中封装了对 **Compressor** 使用过程的通用实现：

```java
public class CompressorStream extends CompressionOutputStream {
     @Override
  public void write(byte[] b, int off, int len) throws IOException {
    //...
    compressor.setInput(b, off, len);
    while (!compressor.needsInput()) {
      compress();
    }
  }
  protected void compress() throws IOException {
    int len = compressor.compress(buffer, 0, buffer.length);
    if (len > 0) {
      out.write(buffer, 0, len);
    }
  }
  public void finish() throws IOException {
    if (!compressor.finished()) {
      compressor.finish();
      while (!compressor.finished()) {
        compress();
      }
    }
  }
  public void close() throws IOException {
    if (!closed) {
      try {
        finish();
      }
      finally {
        out.close();
        closed = true;
      }
    }
  }
}
```

> **Compressor** 四个重要方法的具体实现，这里使用 **SnappyCompressor** 来讲解

- 首先会 **Compressor.setInput()** 输入数据：

```java
 public synchronized void setInput(byte[] b, int off, int len) {
    // .....
    finished = false;
    if (len > uncompressedDirectBuf.remaining()) {
      // 内部缓冲区存不下，则借用外部缓冲区
      this.userBuf = b;
      this.userBufOff = off;
      this.userBufLen = len;
    } else {
     // 内部缓冲区可以存在，则将外部缓冲区数据拷贝进内部缓冲区
      ((ByteBuffer) uncompressedDirectBuf).put(b, off, len);
      uncompressedDirectBufLen = uncompressedDirectBuf.position();
    }
    bytesRead += len;
  }
```

- 然后会调用 **Compressor.needsInput()** 判断内部缓冲区是否已满，四种情况会返回 false：

```java
public synchronized boolean needsInput() {
  // 1. 保存压缩结果的内部缓冲区compressedDirectBuf有未被读取的数据
  // 2. 保存未压缩的内部缓冲区uncompressedDirectBuf 已满
  // 3. 已经“借用”外部缓冲区
    return !(compressedDirectBuf.remaining() > 0
        || uncompressedDirectBuf.remaining() == 0 || userBufLen > 0);
  }
```

- 如果 **Compressor.needsInput()** 返回 false，则需要调用 **Compressor.compress()** 方法，用于获取压缩后的数据，清理内部缓冲区：

```java
public synchronized int compress(byte[] b, int off, int len)
      throws IOException {
    // ......
    // 首先清理 compressedDirectBuf 缓冲区
    int n = compressedDirectBuf.remaining();
    if (n > 0) {
      n = Math.min(n, len);
      ((ByteBuffer) compressedDirectBuf).get(b, off, n);
      bytesWritten += n;
      return n;
    }
    // Re-initialize the snappy's output direct-buffer
    compressedDirectBuf.clear();
    compressedDirectBuf.limit(0);
    if (0 == uncompressedDirectBuf.position()) {
      // 内部未压缩缓冲区没有数据，则从外部缓冲区中拷贝数据到内部未压缩缓冲区
      setInputFromSavedData();
      if (0 == uncompressedDirectBuf.position()) {
        // 没有数据可以处理，设置标记位，并返回0
        finished = true;
        return 0;
      }
    }
    // 压缩数据
    n = compressBytesDirect();
    compressedDirectBuf.limit(n);
    uncompressedDirectBuf.clear();
    // 本地方法已经处理完所有数据，设置标记位
    if (0 == userBufLen) {
      finished = true;
    }
    // 将压缩后的数据拷贝到输出缓冲区
    n = Math.min(n, len);
    bytesWritten += n;
    ((ByteBuffer) compressedDirectBuf).get(b, off, n);
    //返回输出到外部缓冲区的数据大小
    return n;
  }
// 从外部缓冲区中拷贝数据到内部未压缩缓冲区
synchronized void setInputFromSavedData() {
    if (0 >= userBufLen) {
      return;
    }
    finished = false;
    uncompressedDirectBufLen = Math.min(userBufLen, directBufferSize);
    ((ByteBuffer) uncompressedDirectBuf).put(userBuf, userBufOff,
        uncompressedDirectBufLen);
    // Note how much data is being fed to snappy
    userBufOff += uncompressedDirectBufLen;
    userBufLen -= uncompressedDirectBufLen;
  }
```

- **Compressor.finished()** 判断压缩过程是否结束

```java
// finish == true：表示外部不会再输入数据
// finished == true：表示已经输入的数据全部完成压缩
// compressedDirectBuf.remaining() == 0 : 表示压缩完的数据已经全部被读取
public synchronized boolean finished() {
    return (finish && finished && compressedDirectBuf.remaining() == 0);
  }
```

---

> 总结：

1. **CompressionCodecFactory** 工厂类，采用了 **工厂方法** 的设计模式，可以根据应用程序的要求返回对应的 **CompressionCodec** 实现类的对象。
2. Hadoop 压缩框架核心接口为 **CompressionCodec**， 这里使用了 **抽象工厂** 设计模式，每种压缩算法必须实现这个接口，此接口中存在四个方法可以返回压缩算法对应的四个对象： **Compressor** 、**Decompressor**、**CompressionOutputStream**、**CompressionInputStream**。
3. 每种压缩算法需要实现 **Compressor** 和 **Decompressor** 两个接口，分别完成 压缩 和 解压缩 的具体算法实现。
4. 每种压缩算法需要实现 **CompressionOutputStream** 和 **CompressionInputStream** 两个接口，分别实现该压缩算法对流的支持，在这两个接口的实现类中分别会使用对应的 **Compressor** 和 **Decompressor** 实现类的对象。
5. **CompressorStream** 和 **DecompressorStream** 两个类分别是 **CompressionOutputStream** 和 **CompressionInputStream** 的实现类，是对 **Compressor** 和 **Decompressor** 使用过程的通用封装。
6. 使用Hadoop压缩框架的流程：首先通过 **CompressionCodecFactory** 工厂类获得需要的压缩算法所对应的**CompressionCodec**对象，然后通过**CompressionCodec**对象获得**CompressionOutputStream**和**CompressionInputStream**的对象完成对目标流对象的压缩或者解压缩（这两个对象中完成了对**Compressor** 和 **Decompressor**使用过程细节的封装）。

